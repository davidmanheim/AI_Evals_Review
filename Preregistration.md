Note: As an adapted best practice, we are filling out a copy of the "PROSPERO" Systematic review preregistration. Because that system is exclusive to medical and similar systematic reviews, this is being done separately.

The initial preregistration was Jul 21. Amendments to the protocol are tracked in the Git repository. Major changes are listed below:
(28-Aug-2025):
- Include 2 additional sources and searches omitted accidentally, which had been planned and performed beforehand.
- Clarify extracted items and reported outcomes based on early results.
- Explain more about the currently planned categorization of results, extraction, and process.

Review title
------------

Systematic Review of AI Evaluations Recommendations

Review objectives
-----------------

- Compile a set of recommended AI Evaluation practices, for use as the basis of a Delphi elicitation of consensus best practices.

Keywords
--------

AI, Large Language Model, Evaluation, Audit, Standards, Best Practices

SEARCHING AND SCREENING
=======================

Searches
--------

Terms: ("Evaluation" OR "Audit*") AND ("Machine Learning" OR "Artificial Intelligence" OR "Language Model*" OR "AI" OR "Model") AND (("best practice*" OR "good practice*" OR guideline* OR standard* OR protocol* OR framework* OR governance OR "EU AI Act" OR "task force")

Google Scholar: First 5 pages\
("Evaluation" OR "Audit*") AND ("Machine Learning" OR "Artificial Intelligence" OR "Language Model*" OR "AI" OR "Model") AND (("best practice*" OR "good practice*" OR guideline* OR standard* OR protocol* OR framework* OR governance OR "EU AI Act" OR "task force")

Since 2020

Arxiv:612 Results

[https://arxiv.org/search/advanced?advanced=&terms-0-term=%E2%80%9CEvaluation%E2%80%9D+OR+%E2%80%9CAudit%2A%E2%80%9D&terms-0-operator=AND&terms-0-field=title&terms-1-term=%E2%80%9CMachine+Learning%E2%80%9D+OR+%E2%80%9CArtificial+Intelligence%E2%80%9D+OR+%E2%80%9CLanguage+Model%2A%E2%80%9D+OR++%E2%80%9CAI%E2%80%9D+OR+%22Model%22&terms-1-operator=AND&terms-1-field=title&classification-physics_archives=all&classification-include_cross_list=include&date-filter_by=all_dates&date-year=&date-from_date=&date-to_date=&date-date_type=submitted_date&abstracts=show&size=200&order=-announced_date_first&start=0](https://arxiv.org/search/advanced?advanced=&terms-0-operator=AND&terms-0-term=%E2%80%9CEvaluation%E2%80%9D+OR+%E2%80%9CAudit*%E2%80%9D&terms-0-field=title&terms-1-operator=AND&terms-1-term=+%E2%80%9CMachine+Learning%E2%80%9D+OR+%E2%80%9CArtificial+Intelligence%E2%80%9D+OR+%E2%80%9CLarge+Language+Model*%E2%80%9D+OR++%E2%80%9CAI%E2%80%9D+OR+%22Model%22&terms-1-field=title&classification-physics_archives=all&classification-include_cross_list=include&date-filter_by=all_dates&date-year=&date-from_date=&date-to_date=&date-date_type=submitted_date&abstracts=show&size=50&order=-announced_date_first)

Elicit.com search, May 26, 2025, Returning the 10 most relevant papers, of 50 found, using the prompt: “How should evaluators and model developers build evaluations of their models, and what characteristics and pitfalls exist?” available publicly here: https://elicit.com/review/4f49fd01-5601-4c08-8686-d924f0780eca

The search results are be supplemented by expert input and recommended citations during the systematic review, to ensure relevant recommendations are included, which will be reported seperately.

Study design
------------

Only studies which were, or prominently included, reviews or compilation of best practices will be included. After initial filtering based on titles, we review papers and exclude those which are only presenting a new evaluation, are non-generalizable and narrow reviews or overviews, and others found to be irrelevant. We also exclude non-English papers, due to the difficult of review and later expert synthesis of recommendations.

ELIGIBILITY CRITERIA
====================

Condition or domain being studied
---------------------------------

Recommendations, best practices, overviews, criticisms, and similar studies of evaluations, audits, and assessments of Machine learning, AI, GenAI, and LLM or related systems.

Context
-------
A wide variety of overlapping practices exist, and while previous syntheses have been performed, this is intended to inform a process for building broader consensus.

OUTCOMES TO BE ANALYSED
=======================

Main outcomes
-------------

Best practices, such as types of evaluation or assessment, requirements, suggestions, concerns, and process design, performance, and follow-up. These will be given descriptive titles for the later expert review and grouping

Additional outcomes
-------------------
Text of the recommendation for later review, link to the original paper, class of assessment (Evaluation, audit, governnce, or other,) the category of the item, and the scope, i.e. type(s) of AI system discussed.

DATA COLLECTION PROCESS
=======================

Data extraction (selection and coding)
--------------------------------------

All reviews with lists of best practices will be selected. This will be done by at least one reviewer, initially screened based on title, then checked based on the abstract and/or full text to confirm relevance.
For selecteded papers, the list of recommendations will be extracted and categorized. (They may be rphrased or generalized.)

Risk of bias (quality) assessment
---------------------------------
We plan that at least two reviewers will review each selected paper for inclusion, and during the grouping and distiallation process, reviewers and other participants will be able to check that the recommendations correctly reflect what the paper suggests.

PLANNED DATA SYNTHESIS
======================

Strategy for data synthesis
---------------------------
We will compile the list of standards or proposed practices from each paper, then convene experts to help deduplicate and group the recommendations. 

Analysis of subgroups or subsets
--------------------------------

We anticipate that we will focus on items relevant to modern models, and may separate some practices which are relevant only in a narrow domain. Specifically, practices relevant only to generative AI, and practices relevant to catastrophic risk evaluation, will be separated

REVIEW AFFILIATION, FUNDING AND PEER REVIEW
===========================================

Review team members
-------------------

-   David Manheim

-   Others to be determined.

Review affiliation
------------------

Consortium (to be named)

Funding source
--------------

ALTER Israel is funding David Manheim's time and work. ALTER's funding come from a variety of sources, including grants fromOpen Philanthropy, Founder's Pledge, the Survival and Flourishing Fund, and the Silicon Valley Community Foundation, and some non-grant income from contracts. We expect that other participants will be funded by their own organizations, or we will fundraise.

Named contact
-------------

David Manheim.\
david@alter.org.il
